{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mirkocasu/mirkocasu.github.io/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkifJR7AP-Nm"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g50l18biP-No"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUAn_i01P-Np"
      },
      "source": [
        "\n",
        "Introducing FP8 precision training for faster RL inference. [Read Blog](https://docs.unsloth.ai/new/fp8-reinforcement-learning).\n",
        "\n",
        "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
        "\n",
        "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
        "\n",
        "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlB93d_yP-Nq"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNqxK5UoP-Nr"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, importlib.util\n",
        "!pip install --upgrade -qqq uv\n",
        "if importlib.util.find_spec(\"torch\") is None or \"COLAB_\" in \"\".join(os.environ.keys()):\n",
        "    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n",
        "    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n",
        "    !uv pip install -qqq \\\n",
        "        \"torch>=2.8.0\" \"triton>=3.4.0\" {get_numpy} {get_pil} torchvision bitsandbytes \"transformers==4.56.2\" \\\n",
        "        \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
        "        \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
        "        git+https://github.com/triton-lang/triton.git@0add68262ab0a2e33b84524346cb27cbb2787356#subdirectory=python/triton_kernels\n",
        "elif importlib.util.find_spec(\"unsloth\") is None:\n",
        "    !uv pip install -qqq unsloth\n",
        "!uv pip install --upgrade --no-deps transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJq3z_gYnVgd"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2v_X2fA0Df5"
      },
      "source": [
        "We're about to demonstrate the power of the new OpenAI GPT-OSS 20B model through a finetuning example. To use our `MXFP4` inference example, use this [notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/GPT_OSS_MXFP4_(20B)-Inference.ipynb) instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 1024\n",
        "dtype = None\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\", # 20B model using bitsandbytes 4bit quantization\n",
        "    \"unsloth/gpt-oss-120b-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gpt-oss-20b\", # 20B model using MXFP4 format\n",
        "    \"unsloth/gpt-oss-120b\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/gpt-oss-20b\",\n",
        "    dtype = dtype, # None for auto detection\n",
        "    max_seq_length = max_seq_length, # Choose any for long context!\n",
        "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
        "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
        "    # token = \"hf_...\", # use one if using gated models\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVqtZVxxnVgf"
      },
      "source": [
        "We now add LoRA adapters for parameter efficient finetuning - this allows us to only efficiently train 1% of all parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_LK81NRnVgg"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-sFShVvnVgg"
      },
      "source": [
        "### Reasoning Effort\n",
        "The `gpt-oss` models from OpenAI include a feature that allows users to adjust the model's \"reasoning effort.\" This gives you control over the trade-off between the model's performance and its response speed (latency) which by the amount of token the model will use to think.\n",
        "\n",
        "----\n",
        "\n",
        "The `gpt-oss` models offer three distinct levels of reasoning effort you can choose from:\n",
        "\n",
        "* **Low**: Optimized for tasks that need very fast responses and don't require complex, multi-step reasoning.\n",
        "* **Medium**: A balance between performance and speed.\n",
        "* **High**: Provides the strongest reasoning performance for tasks that require it, though this results in higher latency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxCi64FnnVgh"
      },
      "outputs": [],
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Solve x^5 + 3x^4 - 10 = 3.\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        "    return_dict = True,\n",
        "    reasoning_effort = \"low\", # **NEW!** Set reasoning effort to low, medium or high\n",
        ").to(\"cuda\")\n",
        "\n",
        "_ = model.generate(**inputs, max_new_tokens = 64, streamer = TextStreamer(tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlAzq_RinVgh"
      },
      "source": [
        "Changing the `reasoning_effort` to `medium` will make the model think longer. We have to increase the `max_new_tokens` to occupy the amount of the generated tokens but it will give better and more correct answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaPPyXN1nVgh"
      },
      "outputs": [],
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Solve x^5 + 3x^4 - 10 = 3.\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        "    return_dict = True,\n",
        "    reasoning_effort = \"medium\", # **NEW!** Set reasoning effort to low, medium or high\n",
        ").to(\"cuda\")\n",
        "\n",
        "_ = model.generate(**inputs, max_new_tokens = 64, streamer = TextStreamer(tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0iuyJt7nVgh"
      },
      "source": [
        "Lastly we will test it using `reasoning_effort` to `high`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrjUXjN8nVgh"
      },
      "outputs": [],
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Solve x^5 + 3x^4 - 10 = 3.\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        "    return_dict = True,\n",
        "    reasoning_effort = \"high\", # **NEW!** Set reasoning effort to low, medium or high\n",
        ").to(\"cuda\")\n",
        "\n",
        "_ = model.generate(**inputs, max_new_tokens = 64, streamer = TextStreamer(tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6BnnYcbnVgh"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91gfk9L3nVgh"
      },
      "source": [
        "The `HuggingFaceH4/Multilingual-Thinking` dataset will be utilized as our example. This dataset, available on Hugging Face, contains reasoning chain-of-thought examples derived from user questions that have been translated from English into four other languages. It is also the same dataset referenced in OpenAI's [cookbook](https://cookbook.openai.com/articles/gpt-oss/fine-tune-transfomers) for fine-tuning. The purpose of using this dataset is to enable the model to learn and develop reasoning capabilities in these four distinct languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62QfuPXBnVgi"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are a classifier for a virtual assistant.\\n\"\n",
        "    \"Given the conversation context (including previous turns and their annotated states), \"\n",
        "    \"predict the target turn's mental states.\\n\"\n",
        "    \"If the target turn is by the assistant, also predict assistant_safety and assistant_failure_mode.\\n\"\n",
        "    \"If the next speaker is user (i.e., target turn is assistant and next turn exists and is user), \"\n",
        "    \"also forecast next_user_forecast.\\n\\n\"\n",
        "    \"Return ONLY valid JSON with the exact schema requested.\"\n",
        ")\n",
        "\n",
        "def _labels_top3(obj):\n",
        "    # obj è annotation_user_states oppure annotation_assistant_states\n",
        "    if obj is None:\n",
        "        return []\n",
        "    # nel tuo schema: {\"labels_top3\":[{\"state\":..., \"score_1to5\":...}, ...], \"score_scale\":\"1-5\"}\n",
        "    return obj.get(\"labels_top3\", []) or []\n",
        "\n",
        "def _context_line(msg):\n",
        "    role = msg[\"role\"]\n",
        "    content = msg.get(\"content\", \"\")\n",
        "\n",
        "    if role == \"user\":\n",
        "        prev = {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": content,\n",
        "            \"user_states\": _labels_top3(msg.get(\"annotation_user_states\")),\n",
        "        }\n",
        "    else:\n",
        "        prev = {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": content,\n",
        "            \"assistant_states\": _labels_top3(msg.get(\"annotation_assistant_states\")),\n",
        "            \"assistant_safety\": msg.get(\"assistant_safety\"),\n",
        "            \"assistant_failure_mode\": msg.get(\"assistant_failure_mode\"),\n",
        "        }\n",
        "    return prev\n",
        "\n",
        "def build_examples_from_raw(raw_split, max_prev_turns=5):\n",
        "    # raw_split: Dataset split con righe \"message-level\"\n",
        "    by_conv = defaultdict(list)\n",
        "    for r in raw_split:\n",
        "        by_conv[r[\"conversation_id\"]].append(r)\n",
        "\n",
        "    examples = []\n",
        "    for conv_id, msgs in by_conv.items():\n",
        "        msgs = sorted(msgs, key=lambda x: x[\"turn_id\"])\n",
        "\n",
        "        for i, target in enumerate(msgs):\n",
        "            prev_msgs = msgs[max(0, i - max_prev_turns): i]\n",
        "            context = [_context_line(m) for m in prev_msgs]\n",
        "\n",
        "            # Prompt: contesto (con labels precedenti) + turno target (senza labels target)\n",
        "            target_desc = {\n",
        "                \"role\": target[\"role\"],\n",
        "                \"content\": target.get(\"content\", \"\"),\n",
        "            }\n",
        "\n",
        "            user_prompt = json.dumps(\n",
        "                {\n",
        "                    \"conversation_id\": conv_id,\n",
        "                    \"target_turn_id\": target[\"turn_id\"],\n",
        "                    \"context_previous_turns\": context,\n",
        "                    \"target_turn\": target_desc,\n",
        "                    \"task\": {\n",
        "                        \"predict_current_turn_states\": True,\n",
        "                        \"predict_assistant_safety_and_failure_if_applicable\": True,\n",
        "                        \"forecast_next_user_states_if_applicable\": True,\n",
        "                        \"top_k\": 3,\n",
        "                        \"score_scale\": \"1-5\",\n",
        "                    },\n",
        "                    \"output_schema\": {\n",
        "                        \"current_turn\": {\n",
        "                            \"role\": \"user|assistant\",\n",
        "                            \"user_states\": [{\"state\": \"str\", \"score_1to5\": \"int\"}],\n",
        "                            \"assistant_states\": [{\"state\": \"str\", \"score_1to5\": \"int\"}],\n",
        "                            \"assistant_safety\": \"safe|borderline|harmful|null\",\n",
        "                            \"assistant_failure_mode\": \"str|null\",\n",
        "                        },\n",
        "                        \"next_user_forecast\": {\n",
        "                            \"available\": \"bool\",\n",
        "                            \"labels_top3\": [{\"state\": \"str\", \"score_1to5\": \"int\"}],\n",
        "                        },\n",
        "                    },\n",
        "                },\n",
        "                ensure_ascii=False\n",
        "            )\n",
        "\n",
        "            # Ground truth output (supervision)\n",
        "            gt_current = {\n",
        "                \"role\": target[\"role\"],\n",
        "                \"user_states\": _labels_top3(target.get(\"annotation_user_states\")) if target[\"role\"] == \"user\" else [],\n",
        "                \"assistant_states\": _labels_top3(target.get(\"annotation_assistant_states\")) if target[\"role\"] == \"assistant\" else [],\n",
        "                \"assistant_safety\": target.get(\"assistant_safety\") if target[\"role\"] == \"assistant\" else None,\n",
        "                \"assistant_failure_mode\": target.get(\"assistant_failure_mode\") if target[\"role\"] == \"assistant\" else None,\n",
        "            }\n",
        "\n",
        "            # Forecast: solo se target è assistant e il prossimo è user\n",
        "            forecast_available = False\n",
        "            forecast_labels = []\n",
        "            if target[\"role\"] == \"assistant\" and (i + 1) < len(msgs) and msgs[i + 1][\"role\"] == \"user\":\n",
        "                forecast_available = True\n",
        "                forecast_labels = _labels_top3(msgs[i + 1].get(\"annotation_user_states\"))\n",
        "\n",
        "            gt = {\n",
        "                \"current_turn\": gt_current,\n",
        "                \"next_user_forecast\": {\n",
        "                    \"available\": forecast_available,\n",
        "                    \"labels_top3\": forecast_labels if forecast_available else [],\n",
        "                },\n",
        "            }\n",
        "\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": user_prompt},\n",
        "                {\"role\": \"assistant\", \"content\": json.dumps(gt, ensure_ascii=False)},\n",
        "            ]\n",
        "            examples.append({\"messages\": messages})\n",
        "\n",
        "    return examples\n",
        "\n",
        "# Costruzione split train/validation (se presente)\n",
        "train_examples = build_examples_from_raw(raw[\"train\"], max_prev_turns=5)\n",
        "\n",
        "from datasets import Dataset\n",
        "dataset = Dataset.from_list(train_examples)\n",
        "\n",
        "if \"validation\" in raw:\n",
        "    valid_examples = build_examples_from_raw(raw[\"validation\"], max_prev_turns=5)\n",
        "    eval_dataset = Dataset.from_list(valid_examples)\n",
        "else:\n",
        "    eval_dataset = None\n",
        "\n",
        "dataset, eval_dataset\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Cambia questi path (Colab: /content/... oppure Google Drive mountato)\n",
        "TRAIN_JSONL = \"/content/empathy_conv_v1_cleaned_with_content_colab.jsonl\"\n",
        "VALID_JSONL = None  # es. \"/content/valid.jsonl\"\n",
        "\n",
        "data_files = {\"train\": TRAIN_JSONL}\n",
        "if VALID_JSONL is not None:\n",
        "    data_files[\"validation\"] = VALID_JSONL\n",
        "\n",
        "raw = load_dataset(\"json\", data_files=data_files)\n",
        "raw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoVaL_i-nVgj"
      },
      "source": [
        "To format our dataset, we will apply our version of the GPT OSS prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FW-l11GBnVgj"
      },
      "outputs": [],
      "source": [
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"messages\"]\n",
        "    texts = [\n",
        "        tokenizer.apply_chat_template(\n",
        "            convo,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False,\n",
        "        )\n",
        "        for convo in convos\n",
        "    ]\n",
        "    return {\"text\": texts}\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "if eval_dataset is not None:\n",
        "    eval_dataset = eval_dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "dataset[0][\"text\"][:500]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1EmcWNinVgj"
      },
      "source": [
        "Let's take a look at the dataset, and check what the 1st example shows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNgkE5QxnVgj"
      },
      "outputs": [],
      "source": [
        "print(dataset[0]['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ3i-AMFnVgj"
      },
      "source": [
        "What is unique about GPT-OSS is that it uses OpenAI [Harmony](https://github.com/openai/harmony) format which support conversation structures, reasoning output, and tool calling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rtdsxyl6nVgk"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-XZLeLYnVgk"
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    eval_dataset=eval_dataset,   # <-- aggiunta (se non None)\n",
        "    args=SFTConfig(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        num_train_epochs=1,\n",
        "        max_steps=30,\n",
        "        learning_rate=2e-4,\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.001,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWFm3PHLP-N3"
      },
      "source": [
        "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs. This helps increase accuracy of finetunes and lower loss as well!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iitwbjx-P-N3"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "\n",
        "gpt_oss_kwargs = dict(instruction_part = \"<|start|>user<|message|>\", response_part=\"<|start|>assistant<|channel|>final<|message|>\")\n",
        "\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    **gpt_oss_kwargs,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8mnT0ufP-N4"
      },
      "source": [
        "Let's verify masking the instruction part is done! Let's print the 100th row again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxDY1UjEP-N4"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(trainer.train_dataset[100][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L5vaAKLP-N4"
      },
      "source": [
        "Now let's print the masked out example - you should see only the answer is present:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOmh_pbCP-N4"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100][\"labels\"]]).replace(tokenizer.pad_token, \" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJGRuJINP-N5"
      },
      "outputs": [],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92pcbQuKP-N5"
      },
      "source": [
        "Let's train the model! To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFaejiSonVgk"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_G3eBV3EnVgk"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuK0hVOsnVgk"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdVCmTuBnVgl"
      },
      "outputs": [],
      "source": [
        "from transformers import TextStreamer\n",
        "import json\n",
        "\n",
        "def make_classifier_messages(conversation_id, target_turn_id, context_previous_turns, target_turn):\n",
        "    user_payload = {\n",
        "        \"conversation_id\": conversation_id,\n",
        "        \"target_turn_id\": target_turn_id,\n",
        "        \"context_previous_turns\": context_previous_turns,  # lista dict come in training\n",
        "        \"target_turn\": target_turn,                        # {\"role\": \"...\", \"content\": \"...\"}\n",
        "        \"task\": {\n",
        "            \"predict_current_turn_states\": True,\n",
        "            \"predict_assistant_safety_and_failure_if_applicable\": True,\n",
        "            \"forecast_next_user_states_if_applicable\": True,\n",
        "            \"top_k\": 3,\n",
        "            \"score_scale\": \"1-5\",\n",
        "        },\n",
        "        \"output_schema\": {\n",
        "            \"current_turn\": {\n",
        "                \"role\": \"user|assistant\",\n",
        "                \"user_states\": [{\"state\": \"str\", \"score_1to5\": \"int\"}],\n",
        "                \"assistant_states\": [{\"state\": \"str\", \"score_1to5\": \"int\"}],\n",
        "                \"assistant_safety\": \"safe|borderline|harmful|null\",\n",
        "                \"assistant_failure_mode\": \"str|null\",\n",
        "            },\n",
        "            \"next_user_forecast\": {\n",
        "                \"available\": \"bool\",\n",
        "                \"labels_top3\": [{\"state\": \"str\", \"score_1to5\": \"int\"}],\n",
        "            },\n",
        "        },\n",
        "        \"constraints\": [\n",
        "            \"Return ONLY JSON.\",\n",
        "            \"Use top3 lists (0..3 items).\",\n",
        "            \"Use integer score_1to5.\",\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": json.dumps(user_payload, ensure_ascii=False)},\n",
        "    ]\n",
        "\n",
        "# ESEMPIO: compila tu con i tuoi ultimi 5 turni (qui mock minimale)\n",
        "context_previous_turns = [\n",
        "    {\"role\": \"user\", \"content\": \"I hate it when ...\", \"user_states\": [{\"state\":\"embarrassment\",\"score_1to5\":5}]},\n",
        "    {\"role\": \"assistant\", \"content\": \"Ohh! that's so awkward!\", \"assistant_states\": [{\"state\":\"empathy\",\"score_1to5\":3}],\n",
        "     \"assistant_safety\": \"safe\", \"assistant_failure_mode\": \"none\"},\n",
        "]\n",
        "\n",
        "target_turn = {\"role\": \"assistant\", \"content\": \"Yes! Maybe you should set a reminder next time! ha ha!\"}\n",
        "\n",
        "messages = make_classifier_messages(\n",
        "    conversation_id=\"empathy_002295_2a38f246d42e\",\n",
        "    target_turn_id=4,\n",
        "    context_previous_turns=context_previous_turns[-5:],\n",
        "    target_turn=target_turn,\n",
        ")\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\",\n",
        "    return_dict=True,\n",
        ").to(\"cuda\")\n",
        "\n",
        "streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, max_new_tokens=256, streamer=streamer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e1j8KRb4AwO"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** Currently finetunes can only be loaded via Unsloth in the meantime - we're working on vLLM and GGUF exporting!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ds7ByU7e4KF7"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"finetuned_model\")\n",
        "# model.push_to_hub(\"hf_username/finetuned_model\", token = \"hf_...\") # Save to HF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELyXzRpl4hr0"
      },
      "source": [
        "To run the finetuned model, you can do the below after setting `if False` to `if True` in a new instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCMDSxvD4SKu"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"finetuned_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = 1024,\n",
        "        dtype = None,\n",
        "        load_in_4bit = True,\n",
        "    )\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"reasoning language: French\\n\\nYou are a helpful assistant that can solve mathematical problems.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Solve x^5 + 3x^4 - 10 = 3.\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        "    return_dict = True,\n",
        "    reasoning_effort = \"high\",\n",
        ").to(\"cuda\")\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(**inputs, max_new_tokens = 64, streamer = TextStreamer(tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1QU4v6OP-OC"
      },
      "source": [
        "### Saving to float16 for VLLM or mxfp4\n",
        "\n",
        "We also support saving to `float16` or `mxfp4` directly. Select `merged_16bit` for float16. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDCsqu1-P-OD"
      },
      "outputs": [],
      "source": [
        "# Merge and push to hub in mxfp4 4bit format\n",
        "if False:\n",
        "    model.save_pretrained_merged(\"finetuned_model\", tokenizer, save_method = \"mxfp4\")\n",
        "if False: model.push_to_hub_merged(\"repo_id/repo_name\", tokenizer, token = \"hf...\", save_method = \"mxfp4\")\n",
        "\n",
        "# Merge and push to hub in 16bit\n",
        "if False:\n",
        "    model.save_pretrained_merged(\"finetuned_model\", tokenizer, save_method = \"merged_16bit\")\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_merged(\"hf/gpt-oss-finetune\", tokenizer, save_method = \"merged_16bit\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMNviX7XnVgl"
      },
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n",
        "</div>\n",
        "\n",
        "  This notebook and all Unsloth notebooks are licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}